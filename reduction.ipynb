{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-necklace",
   "metadata": {},
   "source": [
    "Reading and observing the dataset at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house prices.csv')\n",
    "df.drop('Id', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nans():\n",
    "    nan_df = pd.DataFrame(df.isna().sum(axis=0), columns=['nan-count'])\n",
    "    nan_df = nan_df.sort_values('nan-count', ascending=False)\n",
    "    return nan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-mitchell",
   "metadata": {},
   "source": [
    "Removing Nan values if possible and replacing them with the Nan values defined specifically for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PoolQC'].fillna('NA', inplace=True)\n",
    "df['MiscFeature'].fillna('NA', inplace=True)\n",
    "df['Alley'].fillna('NA', inplace=True)\n",
    "df['Fence'].fillna('NA', inplace=True)\n",
    "df['FireplaceQu'].fillna('NA', inplace=True)\n",
    "df['GarageCond'].fillna('NA', inplace=True)\n",
    "df['GarageFinish'].fillna('NA', inplace=True)\n",
    "df['GarageQual'].fillna('NA', inplace=True)\n",
    "df['GarageType'].fillna('NA', inplace=True)\n",
    "df['BsmtFinType2'].fillna('NA', inplace=True)\n",
    "df['BsmtFinType1'].fillna('NA', inplace=True)\n",
    "df['BsmtExposure'].fillna('NA', inplace=True)\n",
    "df['BsmtCond'].fillna('NA', inplace=True)\n",
    "df['BsmtQual'].fillna('NA', inplace=True)\n",
    "df['LotFrontage'].fillna(df['LotFrontage'].mean(), inplace=True) # should be experimented\n",
    "show_nans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('GarageYrBlt', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_df = df.select_dtypes('number')\n",
    "object_df = df.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_df.shape, object_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in object_df.columns:\n",
    "    object_df[column] = pd.factorize(object_df[column])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([number_df, object_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df['SalePrice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_features(df, log_target=False):\n",
    "    df_copy = df.copy()\n",
    "    df_copy_target = df_copy.pop('SalePrice')\n",
    "    if log_target:\n",
    "        df_copy_target = np.log1p(df_copy_target)\n",
    "    df_copy_target.reset_index(drop=True, inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df_copy = pd.DataFrame(scaler.fit_transform(df_copy), columns=df_copy.columns)\n",
    "    df_copy = pd.concat([df_copy, df_copy_target], axis=1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-idaho",
   "metadata": {},
   "source": [
    "### First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df.corr(), mask=np.triu(df.corr()), cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = scaler_features(df, log_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = d.pop('SalePrice')\n",
    "X_df = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-commerce",
   "metadata": {},
   "source": [
    "Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = regressor.intercept_\n",
    "features = pd.DataFrame(regressor.coef_, X_df.columns, columns=['coefficient'])\n",
    "features.sort_values('coefficient', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.coefficient = features.coefficient.abs()\n",
    "stdevs = []\n",
    "for i in X_df.columns:\n",
    "    stdev = d[i].std()\n",
    "    stdevs.append(stdev)\n",
    "\n",
    "features[\"stdev\"] = np.array(stdevs).reshape(-1,1)\n",
    "features[\"importance\"] = features[\"coefficient\"] * features[\"stdev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['importance_normalized'] = 100*features['importance'] / features['importance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.sort_values('importance_normalized', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(features.index, features.importance_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-shopping",
   "metadata": {},
   "source": [
    "Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(max_depth=5)\n",
    "dtr.fit(X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(dtr.tree_.compute_feature_importances(), index=X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(feature_importance.index, feature_importance.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-friday",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-booth",
   "metadata": {},
   "source": [
    "### Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = pd.Series(np.ones(X_df.shape[0]), name='bias')\n",
    "X_df = pd.concat([X_df, bias], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-beatles",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "X_dff = X_df.copy()\n",
    "p_value_threshold = 0.05\n",
    "non_usefull_feature = []\n",
    "while True: \n",
    "    model = sm.OLS(y_df, X_dff)\n",
    "    results = model.fit()\n",
    "    highest_p_value = -np.inf\n",
    "    worst_feature = None\n",
    "    for i in range(len(results.pvalues)):\n",
    "        if results.pvalues[i] > p_value_threshold and results.pvalues[i] > highest_p_value:\n",
    "            worst_feature = results.pvalues.index[i]\n",
    "            highest_p_value = results.pvalues[i]\n",
    "    if not worst_feature:\n",
    "        print(results.pvalues)\n",
    "        break\n",
    "    print('removing {} feature'.format(worst_feature))\n",
    "    non_usefull_feature.append(worst_feature)\n",
    "    X_dff.drop(worst_feature, axis=1, inplace=True)\n",
    "end_time = time.time()\n",
    "print('Time took for the operation of backward elimination: {}s'.format(np.round(end_time - start_time, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-portugal",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataloader(df, target_col, batch_size):\n",
    "    target = torch.tensor(df[target_col].values.astype(np.float32))\n",
    "    data = torch.tensor(df.drop(target_col, axis=1).values.astype(np.float32))\n",
    "\n",
    "    data_tensor = torch.utils.data.TensorDataset(data, target)\n",
    "    data_loader = DataLoader(data_tensor, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-pollution",
   "metadata": {},
   "source": [
    "Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor_nn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, output_size, activation_function):\n",
    "        super(Regressor_nn, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.n_layers = n_layers\n",
    "        self.fcs.append(nn.Linear(input_size, hidden_size[0]))\n",
    "        for i in range(n_layers - 1):\n",
    "            self.fcs.append(nn.Linear(hidden_size[i], hidden_size[i + 1]))\n",
    "        self.fcs.append(nn.Linear(hidden_size[-1], output_size))\n",
    "    def forward(self, x):\n",
    "        for i in range(n_layers):\n",
    "            x = self.activation_function(self.fcs[i](x))\n",
    "        x = self.fcs[-1](x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion_mse, criterion_mae, optimizer):\n",
    "    start_time = time.time()\n",
    "    epochs_loss_mse = []\n",
    "    epochs_loss_mae = []\n",
    "    epochs_loss_mse_test = []\n",
    "    epochs_loss_mae_test = []\n",
    "    for epoch in tqdm(range(n_epochs), leave=False):\n",
    "        batchs_loss_mse = []\n",
    "        batchs_loss_mae = []\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            outputs = model(data)\n",
    "            mse = criterion_mse(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            mse.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mae = criterion_mae(outputs, targets)\n",
    "            batchs_loss_mse.append(mse.item())\n",
    "            batchs_loss_mae.append(mae.item())\n",
    "        \n",
    "        epochs_loss_mse.append(np.mean(batchs_loss_mse))\n",
    "        epochs_loss_mae.append(np.mean(batchs_loss_mae))\n",
    "        batchs_loss_mse_test, batchs_loss_mae_test, _, _ = check_accuracy(model, test_loader, criterion_mse, criterion_mae)\n",
    "        epochs_loss_mse_test.append(batchs_loss_mse_test)\n",
    "        epochs_loss_mae_test.append(batchs_loss_mae_test)\n",
    "    end_time = time.time()\n",
    "    print('Time took for the operation of training: {}s'.format(np.round(end_time - start_time, 3)))\n",
    "    return epochs_loss_mse, epochs_loss_mae, epochs_loss_mse_test, epochs_loss_mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader, criterion_mse, criterion_mae):\n",
    "    model.eval()\n",
    "    outputs_agg = np.array([])\n",
    "    targets_agg = np.array([])\n",
    "    with torch.no_grad():\n",
    "        batchs_loss_mse = []\n",
    "        batchs_loss_mae = []\n",
    "        for batch_idx, (data, targets) in enumerate(loader):\n",
    "            outputs = model(data)\n",
    "            outputs_agg = np.append(outputs_agg, outputs.numpy())\n",
    "            targets_agg = np.append(targets_agg, targets.numpy())\n",
    "            mse = criterion_mse(outputs, targets)\n",
    "            mae = criterion_mae(outputs, targets)\n",
    "            batchs_loss_mse.append(mse.item())     \n",
    "            batchs_loss_mae.append(mae.item())\n",
    "    print('mse error :{}, mae error :{}'.format(mse.item(), mae.item()))\n",
    "    model.train()\n",
    "    return np.mean(batchs_loss_mse), np.mean(batchs_loss_mae), outputs_agg, targets_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(epochs_loss_mse, epochs_loss_mae, epochs_loss_mse_test, epochs_loss_mae_test, title):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(title)\n",
    "    sns.lineplot(data=epochs_loss_mse, label='train data', ax=axs[0])\n",
    "    sns.lineplot(data=epochs_loss_mse_test, label='test data', ax=axs[0])\n",
    "    axs[0].grid()\n",
    "    axs[0].set_title('MSE loss per epoch')\n",
    "    sns.lineplot(data=epochs_loss_mae, label='train data', ax=axs[1])\n",
    "    sns.lineplot(data=epochs_loss_mae_test, label='test data', ax=axs[1])\n",
    "    axs[1].grid()\n",
    "    axs[1].set_title('MAE loss per epoch')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(outputs_agg, targets_agg):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(y=outputs_agg, x=targets_agg)\n",
    "    plt.ylabel('predictions')\n",
    "    plt.xlabel('targets')\n",
    "    plt.ylim((np.min(targets_agg), np.max(targets_agg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dff = scaler_features(df, log_target=True)\n",
    "train_df = dff.sample(frac=.8, random_state=707)\n",
    "test_df = dff.drop(train_df.index)\n",
    "\n",
    "train_loader = to_dataloader(\n",
    "    df=train_df, \n",
    "    target_col='SalePrice', \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_loader = to_dataloader(\n",
    "    df=test_df, \n",
    "    target_col='SalePrice', \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dff.shape[1] - 1\n",
    "n_layers = 4\n",
    "hidden_size = [100, 64, 32, 16]\n",
    "activation_function = F.relu\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-least",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Regressor_nn(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    n_layers=n_layers, \n",
    "    output_size=output_size, \n",
    "    activation_function=activation_function\n",
    ")\n",
    "\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_mae = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "epochs_loss_mse, epochs_loss_mae, epochs_loss_mse_test, epochs_loss_mae_test = train(\n",
    "    model, \n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    criterion_mse,\n",
    "    criterion_mae,\n",
    "    optimizer\n",
    ")\n",
    "plot_losses(\n",
    "    epochs_loss_mse, \n",
    "    epochs_loss_mae,\n",
    "    epochs_loss_mse_test,\n",
    "    epochs_loss_mae_test,\n",
    "    'MSE, {}, and {} layers used for training'.format(str(activation_function).split()[1], str(len(hidden_size)))\n",
    ")\n",
    "_, _, outputs_agg, targets_agg = check_accuracy(model, test_loader, criterion_mse, criterion_mae)\n",
    "plot_predictions(outputs_agg, targets_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dff = df.drop(non_usefull_feature, axis=1)\n",
    "dff = scaler_features(dff, log_target=True)\n",
    "train_df = dff.sample(frac=.8, random_state=707)\n",
    "test_df = dff.drop(train_df.index)\n",
    "\n",
    "train_loader = to_dataloader(\n",
    "    df=train_df, \n",
    "    target_col='SalePrice', \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_loader = to_dataloader(\n",
    "    df=test_df, \n",
    "    target_col='SalePrice', \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dff.shape[1] - 1\n",
    "n_layers = 4\n",
    "hidden_size = [100, 64, 32, 16]\n",
    "activation_function = F.relu\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-universe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Regressor_nn(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    n_layers=n_layers, \n",
    "    output_size=output_size, \n",
    "    activation_function=activation_function\n",
    ")\n",
    "\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_mae = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "epochs_loss_mse, epochs_loss_mae, epochs_loss_mse_test, epochs_loss_mae_test = train(\n",
    "    model, \n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    criterion_mse,\n",
    "    criterion_mae,\n",
    "    optimizer\n",
    ")\n",
    "plot_losses(\n",
    "    epochs_loss_mse, \n",
    "    epochs_loss_mae,\n",
    "    epochs_loss_mse_test,\n",
    "    epochs_loss_mae_test,\n",
    "    'MSE, {}, and {} layers used for training'.format(str(activation_function).split()[1], str(len(hidden_size)))\n",
    ")\n",
    "_, _, outputs_agg, targets_agg = check_accuracy(model, test_loader, criterion_mse, criterion_mae)\n",
    "plot_predictions(outputs_agg, targets_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-necklace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
